{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHmWHmssFWCp"
      },
      "source": [
        "Original dataset has resolution of (224 x 224) pixels\n",
        "\n",
        "it uses EPSG:32611\n",
        "\n",
        "NoData value is -9999.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C2237824918-ORNL_CLOUD\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import affine\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import rioxarray\n",
        "import rasterio as rio\n",
        "from rasterio.transform import from_origin\n",
        "from rasterio.windows import Window\n",
        "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
        "from rasterio.crs import CRS\n",
        "from rasterio.vrt import WarpedVRT\n",
        "import h5py\n",
        "import tabulate\n",
        "import contextily as ctx\n",
        "import numpy as np\n",
        "import pyproj\n",
        "from getpass import getpass\n",
        "from IPython.display import HTML, display\n",
        "from shapely.geometry import MultiPolygon, Polygon, box\n",
        "from shapely.ops import orient\n",
        "from pyproj import Transformer\n",
        "import os\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import glob\n",
        "import json\n",
        "import earthaccess\n",
        "import requests\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from shapely.geometry import MultiPolygon, Polygon, box\n",
        "\n",
        "# Write the credentials to the .netrc file\n",
        "# Get user's home directory\n",
        "home_dir = os.path.expanduser('~')\n",
        "netrc_path = os.path.join(home_dir, '.netrc')\n",
        "\n",
        "# Write the credentials to the .netrc file\n",
        "with open(netrc_path, 'w') as f:\n",
        "    f.write(\"machine urs.earthdata.nasa.gov login jasonip password EarthMining_101\\n\")\n",
        "\n",
        "# Set file permissions to secure the credentials using os module instead of shell command\n",
        "os.chmod(netrc_path, 0o600)\n",
        "\n",
        "doi = '10.3334/ORNLDAAC/2056'# GEDI L4A DOI\n",
        "doiS2 = '10.5067/HLS/HLSS30.002' # S2 DOI\n",
        "\n",
        "# CMR API base url\n",
        "cmrurl='https://cmr.earthdata.nasa.gov/search/'\n",
        "\n",
        "doisearch = cmrurl + 'collections.json?doi=' + doi\n",
        "response = requests.get(doisearch)\n",
        "response.raise_for_status()\n",
        "concept_id = response.json()['feed']['entry'][0]['id']\n",
        "\n",
        "print(concept_id)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def search_gedi_granules_poly(concept_id, polygon, start_date, end_date, cmrurl='https://cmr.earthdata.nasa.gov/search/', data_amount=1):\n",
        "    # Calculate the bounding box from the polygon for initial broad search\n",
        "    min_lon, min_lat, max_lon, max_lat = polygon.bounds\n",
        "    bound = (min_lon, min_lat, max_lon, max_lat)\n",
        "\n",
        "    # Format dates and bounding box for CMR API\n",
        "    dt_format = '%Y-%m-%dT%H:%M:%SZ'\n",
        "    temporal_str = start_date.strftime(dt_format) + ',' + end_date.strftime(dt_format)\n",
        "    bound_str = ','.join(map(str, bound))\n",
        "\n",
        "    page_num = 1\n",
        "    page_size = 2000\n",
        "    granule_arr = []\n",
        "\n",
        "    while True:\n",
        "        # ... existing CMR API request code ...\n",
        "        cmr_param = {\n",
        "            \"collection_concept_id\": concept_id,\n",
        "            \"page_size\": page_size,\n",
        "            \"page_num\": page_num,\n",
        "            \"temporal\": temporal_str,\n",
        "            \"bounding_box[]\": bound_str\n",
        "        }\n",
        "\n",
        "        granulesearch = cmrurl + 'granules.json'\n",
        "        response = requests.get(granulesearch, params=cmr_param)\n",
        "        response.raise_for_status()\n",
        "        granules = response.json()['feed']['entry']\n",
        "        if granules:\n",
        "            for g in granules:\n",
        "                granule_url = ''\n",
        "                granule_poly = ''\n",
        "                granule_size = float(g['granule_size'])\n",
        "\n",
        "                # reading bounding geometries\n",
        "                if 'polygons' in g:\n",
        "                    polygons = g['polygons']\n",
        "                    multipolygons = []\n",
        "                    for poly in polygons:\n",
        "                        i = iter(poly[0].split(\" \"))\n",
        "                        ltln = list(map(\" \".join, zip(i, i)))\n",
        "                        multipolygons.append(Polygon([[float(p.split(\" \")[1]), float(p.split(\" \")[0])] for p in ltln]))\n",
        "                    granule_poly = MultiPolygon(multipolygons)\n",
        "\n",
        "                    # Only add granule if it intersects with the input polygon\n",
        "\n",
        "                    if granule_poly.intersects(polygon) and granule_poly.within(polygon):\n",
        "                        # Get URL to HDF5 files\n",
        "                        for links in g['links']:\n",
        "                            if 'title' in links and links['title'].startswith('Download') \\\n",
        "                            and links['title'].endswith('.h5'):\n",
        "                                granule_url = links['href']\n",
        "                        granule_arr.append([granule_url, granule_size, granule_poly])\n",
        "\n",
        "            page_num += 1\n",
        "        else:\n",
        "            break\n",
        "    # Create DataFrame without adding bounding box\n",
        "    l4adf = pd.DataFrame(granule_arr, columns=[\"granule_url\", \"granule_size\", \"granule_poly\"])\n",
        "\n",
        "    # Drop granules with empty geometry\n",
        "    l4adf = l4adf[l4adf['granule_poly'] != '']\n",
        "\n",
        "    # drop duplicate URLs if any\n",
        "    l4a_granules = l4adf.drop_duplicates(subset=['granule_url'])\n",
        "    l4a_granules.to_csv('granules.txt', columns=['granule_url'], index=False, header=False)\n",
        "\n",
        "    # Download the files\n",
        "    command = f\"\"\"\n",
        "    head -n {data_amount} granules.txt | tr -d '\\\\r' | xargs -n 1 -I {{}} bash -c 'filename=$(basename {{}}); curl -LJO -n -c ~/.urs_cookies -b ~/.urs_cookies -o \"gedi_folder/$filename\" {{}}; echo $filename'\n",
        "    \"\"\"\n",
        "\n",
        "    result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    downloaded_files = result.stdout.splitlines()\n",
        "\n",
        "    # print(f\"Found {len(l4adf)} granules intersecting with the polygon\")\n",
        "    # print(f\"Total file size (MB): {l4adf['granule_size'].sum()}\")\n",
        "\n",
        "    return l4adf, downloaded_files\n",
        "def to_csv(downloaded_files, output_folder , file_name = None):\n",
        "    hfList = []\n",
        "\n",
        "    # read the L4A files\n",
        "\n",
        "    hfList.append(h5py.File(downloaded_files, 'r'))\n",
        "\n",
        "    # printing root-level groups\n",
        "    list(hfList[0].keys())\n",
        "\n",
        "    # read the METADATA group\n",
        "    metadata = hfList[0]['METADATA/DatasetIdentification']\n",
        "    # store attributes and descriptions in an array\n",
        "    data = []\n",
        "    for attr in metadata.attrs.keys():\n",
        "        data.append([attr, metadata.attrs[attr]])\n",
        "\n",
        "    # display `data` array as a table\n",
        "    tbl_n = 1 # table number\n",
        "    # print(f'Table {tbl_n}. Attributes and discription from `METADATA` group')\n",
        "    headers = [\"attribute\", \"description\"]\n",
        "    # display(HTML(tabulate.tabulate(data, headers, tablefmt='html')))\n",
        "\n",
        "    # read the ANCILLARY group\n",
        "    ancillary = []\n",
        "\n",
        "    for hf in hfList:\n",
        "        ancillary.append(hf['ANCILLARY'])\n",
        "\n",
        "    # read model_data subgroup\n",
        "    model_data = []\n",
        "    for data in ancillary:\n",
        "        model_data.append(data['model_data'])\n",
        "\n",
        "    # initialize an empty dataframe\n",
        "    model_data_df = pd.DataFrame()\n",
        "\n",
        "    first_model = model_data[0]\n",
        "    # loop through parameters\n",
        "    for v in first_model.dtype.names:\n",
        "        # exclude multidimensional variables\n",
        "        if (len(first_model[v].shape) == 1):\n",
        "            # copy parameters as dataframe column\n",
        "            model_data_df[v] = first_model[v]\n",
        "            # converting object datatype to string\n",
        "            if model_data_df[v].dtype.kind=='O':\n",
        "                model_data_df[v] = model_data_df[v].str.decode('utf-8')\n",
        "\n",
        "    # print the parameters\n",
        "    tbl_n += 1\n",
        "    # read pft_lut subgroup\n",
        "    pft_lut = ancillary[0]['pft_lut']\n",
        "    headers = pft_lut.dtype.names\n",
        "    # print pft class and names\n",
        "    data = zip(pft_lut[headers[0]], pft_lut[headers[1]])\n",
        "    # display(HTML(tabulate.tabulate(data, headers, tablefmt='html')))\n",
        "    # read region_lut subgroup\n",
        "    region_lut = ancillary[0]['region_lut']\n",
        "    headers = region_lut.dtype.names\n",
        "    # print region class and names\n",
        "    data = zip(region_lut[headers[0]], region_lut[headers[1]])\n",
        "    # display(HTML(tabulate.tabulate(data, headers, tablefmt='html')))\n",
        "    # index of DBT_NAm predict_stratum, idx = 6\n",
        "    idx = model_data_df[model_data_df['predict_stratum']=='DBT_NAm'].index.item()\n",
        "    # print vcov matrix\n",
        "    model_data[0]['vcov'][idx]\n",
        "    ## get predictor_id, rh_index and par for idx = 6\n",
        "    predictor_id = model_data[0]['predictor_id'][idx]\n",
        "    rh_index = model_data[0]['rh_index'][idx]\n",
        "    par = model_data[0]['par'][idx]\n",
        "\n",
        "    # print\n",
        "    print_s = f\"\"\"predictor_id: {predictor_id}\n",
        "    rh_index: {rh_index}\n",
        "    par: {par}\"\"\"\n",
        "    # print(print_s)\n",
        "\n",
        "    # initialize arrays\n",
        "    stratum_arr, modelname_arr, fitstratum_arr, agbd_arr = [], [], [], []\n",
        "    # loop the model_data_df dataframe\n",
        "    for idx, row in model_data_df.iterrows():\n",
        "        stratum_arr.append(model_data_df['predict_stratum'][idx])\n",
        "        modelname_arr.append(model_data_df['model_name'][idx])\n",
        "        fitstratum_arr.append(model_data_df['fit_stratum'][idx])\n",
        "        i_0 = 0\n",
        "        predictor_id = model_data[0]['predictor_id'][idx]\n",
        "        rh_index = model_data[0]['rh_index'][idx]\n",
        "        par = model_data[0]['par'][idx]\n",
        "        model_str = 'AGBD = ' + str(par[0]) # intercept\n",
        "        for i in predictor_id[predictor_id>0]:\n",
        "            # use product of two RH metrics when consecutive\n",
        "            # predictor_id have same values\n",
        "            if (i == i_0):\n",
        "                model_str += ' x RH_' + str(rh_index[i-1])\n",
        "            # adding slope coefficients\n",
        "            else:\n",
        "                model_str += ' + ' + str(par[i]) + ' x RH_' + str(rh_index[i-1])\n",
        "            i_0 = i\n",
        "        # agbd model\n",
        "        agbd_arr.append(model_str)\n",
        "\n",
        "    # unique agbd models\n",
        "    unique_models = list(set(agbd_arr))\n",
        "\n",
        "    # printing agbd models by predict_stratum\n",
        "    data=[]\n",
        "    for model in unique_models:\n",
        "        s, m, f = [], [], []\n",
        "        for i, x in enumerate(agbd_arr):\n",
        "            if x == model:\n",
        "                s.append(stratum_arr[i])\n",
        "                m.append(modelname_arr[i])\n",
        "                f.append(fitstratum_arr[i])\n",
        "        data.append([\", \".join(s), \", \".join(list(set(m))), \", \".join(list(set(f))), model])\n",
        "    tbl_n += 1\n",
        "    # print(f'Table {tbl_n}. AGBD Linear Models by Prediction Stratum')\n",
        "    headers = [\"predict_stratum\", \"model_name\", \"fit_stratum\", \"AGBD model\"]\n",
        "    # display(HTML(tabulate.tabulate(data, headers, tablefmt='html', stralign=\"left\")))\n",
        "\n",
        "    data = []\n",
        "    # loop through the root groups\n",
        "    for v in list(hfList[0].keys()):\n",
        "        if v.startswith('BEAM'):\n",
        "            beam = hfList[0].get(v)\n",
        "            b_beam = beam.get('beam')[0]\n",
        "            channel = beam.get('channel')[0]\n",
        "            data.append([v, hf[v].attrs['description'], b_beam, channel])\n",
        "\n",
        "    # print as a table\n",
        "    tbl_n += 1\n",
        "    # print(f'Table {tbl_n}. GEDI Beams')\n",
        "    headers = [\"beam name\", \"description\", \"beam\", \"channel\"]\n",
        "    # display(HTML(tabulate.tabulate(data, headers, tablefmt='html')))\n",
        "\n",
        "    beam_str = ['BEAM0101','BEAM0110','BEAM1000', 'BEAM1011']\n",
        "    beam0110 = hf[beam_str[0]]\n",
        "\n",
        "    data = []\n",
        "    # loop over all the variables within BEAM0110 group\n",
        "    for v in beam0110.keys():\n",
        "        var = beam0110[v]\n",
        "        source = ''\n",
        "        # if the key is a subgroup assign GROUP tag\n",
        "        if isinstance(var, h5py.Group):\n",
        "            data.append([v, 'GROUP', 'GROUP', 'GROUP'])\n",
        "        # read source, description, units attributes of each variables\n",
        "        else:\n",
        "            if 'source' in var.attrs.keys():\n",
        "                source = var.attrs['source']\n",
        "            data.append([v, var.attrs['description'], var.attrs['units'], source])\n",
        "\n",
        "    # print all variable name and attributes as a table\n",
        "    tbl_n += 1\n",
        "    # print(f'Table {tbl_n}. Variables within {beam_str} group')\n",
        "    headers = [\"variable\", \"description\", \"units\", \"source\"]\n",
        "    data = sorted(data, key=lambda x:x[3])\n",
        "    # display(HTML(tabulate.tabulate(data, headers, tablefmt='html')))\n",
        "\n",
        "    # Folder to save individual CSV files\n",
        "\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Initialize file counter\n",
        "\n",
        "\n",
        "    for hf in hfList:\n",
        "        # Temporary lists for each hf\n",
        "        elev_l = []\n",
        "        lat_l = []\n",
        "        lon_l = []\n",
        "        agbd_l = []\n",
        "        error_l = []\n",
        "        beam_n = []\n",
        "        time_l = []\n",
        "        quality_l = []\n",
        "\n",
        "        # Loop over all base groups in each hf\n",
        "        for var in list(hf.keys()):\n",
        "            if var.startswith('BEAM'):\n",
        "                beam = hf.get(var)\n",
        "                agbd = beam.get('agbd')[:]\n",
        "                error = beam.get('agbd_se')[:]\n",
        "                elev = beam.get('elev_lowestmode')[:]\n",
        "                lat = beam.get('lat_lowestmode')[:]\n",
        "                lon = beam.get('lon_lowestmode')[:]\n",
        "                time = beam.get('delta_time')[:]\n",
        "                quality = beam.get('l4_quality_flag')[:]\n",
        "\n",
        "                # Append data to temporary lists\n",
        "                agbd_l.extend(agbd.tolist())\n",
        "                error_l.extend(error.tolist())\n",
        "                elev_l.extend(elev.tolist())\n",
        "                lat_l.extend(lat.tolist())\n",
        "                lon_l.extend(lon.tolist())\n",
        "                time_l.extend(time.tolist())\n",
        "                quality_l.extend(quality.tolist())\n",
        "                n = lat.shape[0]\n",
        "                beam_n.extend(np.repeat(str(var), n).tolist())\n",
        "\n",
        "        # Create a DataFrame for the current hf\n",
        "        df_hf = pd.DataFrame(list(zip(beam_n, agbd_l, error_l, elev_l, lat_l, lon_l, time_l, quality_l)),\n",
        "                            columns=[\"beam\", \"agbd\", \"agbd_se\", \"elev_lowestmode\", \"lat_lowestmode\", \"lon_lowestmode\", \"delta_time\", \"l4_quality_flag\"])\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        df_hf.to_csv(os.path.join(output_folder, file_name), index=False)\n",
        "\n",
        "        # Clear the temporary DataFrame and lists to free RAM\n",
        "        del df_hf, elev_l, lat_l, lon_l, agbd_l, error_l, beam_n, time_l, quality_l\n",
        "        # file_counter += 1  # Increment file counter\n",
        "\n",
        "def load_polygons(csv_file):\n",
        "    \"\"\"\n",
        "    Load polygon data from CSV file containing WKT geometry strings.\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): Path to CSV file containing polygon data\n",
        "\n",
        "    Returns:\n",
        "        gpd.GeoDataFrame: GeoDataFrame containing the polygon data\n",
        "    \"\"\"\n",
        "    # Read CSV file\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Convert WKT strings to geometry objects\n",
        "    gdf = gpd.GeoDataFrame(\n",
        "        df,\n",
        "        geometry=gpd.GeoSeries.from_wkt(df['geometry']),\n",
        "        crs=\"EPSG:4326\"  # Assuming coordinates are in WGS84\n",
        "    )\n",
        "\n",
        "    return gdf\n",
        "def get_bboxes_from_polygons(gdf):\n",
        "    \"\"\"\n",
        "    Convert polygons to bounding boxes in the format required for satellite data download.\n",
        "\n",
        "    Args:\n",
        "        gdf (gpd.GeoDataFrame): GeoDataFrame containing polygon geometries\n",
        "\n",
        "    Returns:\n",
        "        list: List of bounding boxes in format [(min_lon, min_lat, max_lon, max_lat), ...]\n",
        "    \"\"\"\n",
        "    bboxes = []\n",
        "    polys = []\n",
        "    for _, polygon in gdf.iterrows():\n",
        "        bounds = polygon.geometry.bounds  # Returns (minx, miny, maxx, maxy)\n",
        "        bbox = (bounds[0], bounds[1], bounds[2], bounds[3])  # (min_lon, min_lat, max_lon, max_lat)\n",
        "        poly = polygon.geometry\n",
        "        polys.append(poly)\n",
        "        bboxes.append(bbox)\n",
        "    return bboxes, polys\n",
        "def generate_weekly_date_pairs(start_date, end_date):\n",
        "    \"\"\"\n",
        "    Generates a list of weekly date pairs (start, end) from start_date to end_date.\n",
        "    Each date is in the format: datetime.datetime(year, month, day, 0, 0).\n",
        "\n",
        "    Args:\n",
        "        start_date: The starting date (datetime object).\n",
        "        end_date: The ending date (datetime object).\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the start and end dates of a week.\n",
        "    \"\"\"\n",
        "    weekly_pairs = []\n",
        "    current_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)  # Set time to 00:00:00\n",
        "    while current_date <= end_date:\n",
        "        end_of_week = current_date + dt.timedelta(days=6)\n",
        "        end_of_week = end_of_week.replace(hour=0, minute=0, second=0, microsecond=0)  # Set time to 00:00:00\n",
        "        weekly_pairs.append((current_date, end_of_week))\n",
        "        current_date += dt.timedelta(weeks=1)\n",
        "    return weekly_pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing sample_0-2022_06_17_to_2022_06_23, sample0:   0%|          | 0/2067 [00:03<?, ?it/s]         \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m bar\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Search and download GEDI data\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m l4adf, downloaded_files \u001b[38;5;241m=\u001b[39m \u001b[43msearch_gedi_granules_poly\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcept_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_amount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_amount\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(downloaded_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m bar\u001b[38;5;241m.\u001b[39mupdate()\n",
            "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36msearch_gedi_granules_poly\u001b[0;34m(concept_id, polygon, start_date, end_date, cmrurl, data_amount)\u001b[0m\n\u001b[1;32m     17\u001b[0m cmr_param \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection_concept_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: concept_id,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: page_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbounding_box[]\u001b[39m\u001b[38;5;124m\"\u001b[39m: bound_str\n\u001b[1;32m     23\u001b[0m }\n\u001b[1;32m     25\u001b[0m granulesearch \u001b[38;5;241m=\u001b[39m cmrurl \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgranules.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 26\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgranulesearch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmr_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     28\u001b[0m granules \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeed\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentry\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[0;32m~/anaconda3/envs/granite_env/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/granite_env/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/granite_env/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/anaconda3/envs/granite_env/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
            "File \u001b[0;32m~/anaconda3/envs/granite_env/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
            "File \u001b[0;32m~/anaconda3/envs/granite_env/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/granite_env/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
            "File \u001b[0;32m~/anaconda3/envs/granite_env/lib/python3.11/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
            "File \u001b[0;32m~/anaconda3/envs/granite_env/lib/python3.11/http/client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/anaconda3/envs/granite_env/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/granite_env/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/granite_env/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/granite_env/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m~/anaconda3/envs/granite_env/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "# bound = (-9.0, 38.0, -8.5, 38.5)\n",
        "from tqdm import tqdm\n",
        "data_amount = 1\n",
        "polygons = load_polygons('species.csv')\n",
        "bboxes, polys = get_bboxes_from_polygons(polygons)\n",
        "\n",
        "weekly_pairs = generate_weekly_date_pairs(dt.datetime(2022, 4, 1), dt.datetime(2022, 10, 1))\n",
        "\n",
        "destination_dir = \"H5_labels\"\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "bar = tqdm(polys, desc=\"Processing bounding boxes\")\n",
        "total_files = 0\n",
        "for index, bound in enumerate(bar):\n",
        "    for start_date, end_date in weekly_pairs:\n",
        "        \n",
        "        # Generate consistent filename\n",
        "        new_name = f\"sample_{index}-{start_date.strftime('%Y_%m_%d')}_to_{end_date.strftime('%Y_%m_%d')}\"\n",
        "        destination_file = os.path.join(destination_dir, new_name + '.h5')\n",
        "\n",
        "        # Skip if file already exists\n",
        "        if os.path.exists(destination_file):\n",
        "            continue\n",
        "        bar.set_description(f\"Processing {new_name}, sample {index}\")\n",
        "        bar.update()\n",
        "        # Search and download GEDI data\n",
        "        l4adf, downloaded_files = search_gedi_granules_poly(\n",
        "            concept_id, \n",
        "            bound, \n",
        "            start_date, \n",
        "            end_date, \n",
        "            data_amount=data_amount\n",
        "        )\n",
        "        bar.set_description(f\"Found {len(downloaded_files)} files\")\n",
        "        bar.update()\n",
        "        if not downloaded_files:\n",
        "            continue\n",
        "\n",
        "        # Move downloaded file to destination with new name\n",
        "        try:\n",
        "            shutil.move(downloaded_files[0], destination_file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving file: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Convert to CSV\n",
        "        try:\n",
        "            to_csv(\n",
        "                destination_file,\n",
        "                output_folder=\"./csv_labels\",\n",
        "                file_name=new_name + '.csv'\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error converting to CSV: {e}\")\n",
        "            continue\n",
        "        total_files += len(downloaded_files)\n",
        "        bar.set_description(f\"Found {total_files} files for {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
        "        bar.update()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "to_csv(\n",
        "    'sample_58-2022_04_01_to_2022_04_07.h5',\n",
        "    output_folder=\"./csv_labels\",\n",
        "    file_name='sample_58-2022_04_01_to_2022_04_07.csv'\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
